---
title: "MA678 Homework 2"
author: "Chang Lu"
date: "9/20/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 11.5 
*Residuals and predictions*: The folder `Pyth` contains outcome $y$ and predictors $x_1$, $x_2$ for 40 data points, with a further 20 points with the predictors but no observed outcome. Save the file to your working directory, then read it into R using `read.table()`.

### (a) 
Use R to fit a linear regression model predicting $y$ from $x_1$, $x_2$, using the first 40 data points in the file. Summarize the inferences and check the fit of your model.

```{r}
pyth_data <- read.table("pyth.txt",header = TRUE)
pyth_data_subset <- pyth_data[0:40, ]
pyth_model <- lm(y ~ x1 + x2, data = pyth_data_subset)
summary(pyth_model)
pyth_residuals <- residuals(pyth_model)
plot(fitted(pyth_model),pyth_residuals)
abline(h = 0, col="red")

```

### (b) 
Display the estimated model graphically as in Figure 10.2

```{r}
plot(pyth_model)
```

### (c) 
Make a residual plot for this model. Do the assumptions appear to be met?

```{r}
pyth_residuals <- residuals(pyth_model)
plot(pyth_residuals)
abline(h = 0, col="red")

```
1. The residuals seem randomly scattered around the horizontal line (zero), with no clear pattern, which suggests that linearity is met.

2. The spread of residuals seems fairly consistent across the index range, indicating that homoscedasticity is likely met.

As a result, the model assumptions seem to be generally met.

### (d) 
Make predictions for the remaining 20 data points in the file. How confident do you feel about these predictions?

```{r}
newpyth_data <- pyth_data[41:60,]
pyth_prediction <- predict(pyth_model, data = newpyth_data)
pyth_prediction_interval <-  predict(pyth_model, data = newpyth_data, interval = "confidence")
pyth_prediction_interval

```

I would feel moderately confident in these predictions, especially for those with narrower confidence intervals, which indicating high precision and showing the model is quite confident with the predictions.

## 12.5 
*Logarithmic transformation and regression*: Consider the following regression:
$$\log(\text{weight})=-3.8+2.1 \log(\text{height})+\text{error,} $$
with errors that have standard deviation 0.25. Weights are in pounds and heights are in inches.

### (a) 
Fill in the blanks: Approximately 68% of the people will have weights within a factor of 0.778($e^{-0.25}$) and 1.284($e^{0.25}$) of their predicted values from the regression.

### (b) 
Using pen and paper, sketch the regression line and scatterplot of log(weight) versus log(height) that make sense and are consistent with the fitted model. Be sure to label the axes of your graph.

```{r}
library(ggplot2)

height <- seq(50, 80, by = 1) 
log_height <- log(height)

log_weight <- -3.8 + 2.1 * log_height

data <- data.frame(log_height, log_weight)

ggplot(data, aes(x = log_height, y = log_weight)) +
  geom_point(color = 'blue') + 
  geom_line(color = 'red') +   
  labs(x = "log(Height)", y = "log(Weight)") +  
  ggtitle("Regression Line and Scatterplot of log(Weight) vs log(Height)") +
  theme_minimal()
```


## 12.6 
*Logarithmic transformations*: The folder `Pollution` contains mortality rates and various environmental factors from 60 US metropolitan areas. For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. this model is an extreme oversimplication, as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformation in regression.  

### (a) 
Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.

```{r}
pollution_data <- read.csv("pollution.csv")

head(pollution_data)
```

```{r}
ggplot(pollution_data, aes(x = nox, y = mort)) +
  geom_point(color = 'blue') +
  labs(x = "Nitric Oxides (ppm)", y = "Mortality Rate (per 100,000)") +
  ggtitle("Scatterplot of Mortality Rate vs Nitric Oxides") +
  theme_minimal()

model_a <- lm(mort ~ nox, data = pollution_data)

ggplot(data.frame(Residuals = residuals(model_a), Fitted = fitted(model_a)), aes(x = Fitted, y = Residuals)) +
  geom_point(color = 'blue') +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Fitted Values", y = "Residuals") +
  ggtitle("Residual Plot for Mortality vs Nitric Oxides") +
  theme_minimal()
```
From the scatterplot of Mortality Rate vs Nitric Oxides, it is clear that the data does not exhibit a strong linear relationship. Most of the data points are clustered tightly near lower levels of nitric oxides, with some outliers at higher levels, which introduces heteroscedasticity. This suggests that a simple linear regression may not be the best fit for this data due to the presence of non-linearity and potentially influential outliers.

Additionally, the residual plot reveals a pattern where the residuals are not randomly dispersed around zero, especially toward higher fitted values. This is a further indication that the linear model is not adequately capturing the underlying relationship between mortality and nitric oxides. There is also potential non-constant variance (heteroscedasticity), which violates one of the assumptions of linear regression.

Based on this, a linear regression model likely does not fit the data well.

### (b) 
Find an appropriate reansformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.

```{r}
pollution_data$log_nox <- log(pollution_data$nox)

model_b <- lm(mort ~ log_nox, data = pollution_data)

summary(model_b)

ggplot(data.frame(Residuals = residuals(model_b), Fitted = fitted(model_b)), aes(x = Fitted, y = Residuals)) +
  geom_point(color = 'blue') +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Fitted Values (Log Transformed NOx)", y = "Residuals") +
  ggtitle("Residual Plot for Transformed Model: Mortality vs Log(Nitric Oxides)") +
  theme_minimal()

```
The Log-transformed residual plot suggests that the log transformation has helped linearize the relationship between mortality and nitric oxides.

### (c) 
Interpret the slope coefficient from the model you chose in (b)

Intercept (Estimate = 904.724): This is the predicted mortality rate when the log of nitric oxides (log_nox) is zero. Mathematically, this means when nitric oxides is 1, the predicted mortality rate is about 904.724 deaths per 100,000 people.

### (d) 
Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformation when helpful. Plot the fitted regression model and interpret the coefficients.

```{r}
ggplot(pollution_data, aes(x = so2, y = mort)) +
  geom_point(color = 'blue') +
  labs(x = "Sulfur Dioxide (ppm)", y = "Mortality Rate (per 100,000)") +
  ggtitle("Scatterplot of Mortality Rate vs Sulfur Dioxide") +
  theme_minimal()

ggplot(pollution_data, aes(x = hc, y = mort)) +
  geom_point(color = 'blue') +
  labs(x = "Hydrocarbons (ppm)", y = "Mortality Rate (per 100,000)") +
  ggtitle("Scatterplot of Mortality Rate vs Hydrocarbons") +
  theme_minimal()
```

We can defer from two plots above that so2 and hc needs to be transformed.

```{r}
pollution_data$log_so2 <- log(pollution_data$so2)  # log transform sulfur dioxide
pollution_data$log_hc <- log(pollution_data$hc)    # log transform hydrocarbons

# Fit multiple regression model
model_multiple <- lm(mort ~ log_nox + log_so2 + log_hc, data = pollution_data)

# Summary of the model
summary(model_multiple)

# Create a plot of fitted vs actual values
ggplot(pollution_data, aes(x = fitted(model_multiple), y = mort)) +
  geom_point(color = 'blue') +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Fitted Mortality Rate", y = "Actual Mortality Rate") +
  ggtitle("Fitted vs Actual Mortality Rate for Multiple Regression Model") +
  theme_minimal()
```

#### Residuals:

The residuals show how well the model fits the data. The spread from the min to the max shows that some observations are not perfectly predicted. A few residuals are large, particularly the maximum residual (194.567), indicating some points where the model overpredicts significantly.

#### Coefficients:

(a) Intercept (924.965):
The estimated mortality rate when all three log-transformed predictors (log_nox, log_so2, and log_hc) are zero. This means in areas where levels of nitric oxides, sulfur dioxide, and hydrocarbons are very low, the model predicts a baseline mortality rate of around 924.965 per 100,000 people.

(b) log_nox (58.336):
The coefficient for log_nox (nitric oxides) suggests that for a 1% increase in nitric oxide levels, the mortality rate is expected to increase by approximately 58.34 deaths per 100,000, holding the other pollutants constant. This effect is statistically significant with a p-value of 0.0096, meaning there's strong evidence that higher levels of nitric oxides are associated with higher mortality.

(c) log_so2 (11.762):
The coefficient for log_so2 (sulfur dioxide) suggests that for a 1% increase in sulfur dioxide levels, the mortality rate is expected to increase by about 11.76 deaths per 100,000, holding the other pollutants constant. However, the p-value is 0.106, which is not statistically significant at the 5% level. This means there's insufficient evidence to claim a significant association between sulfur dioxide and mortality rates.

(d) log_hc (-57.300):
The coefficient for log_hc (hydrocarbons) is -57.30, indicating that for a 1% increase in hydrocarbon levels, the mortality rate is expected to decrease by about 57.30 deaths per 100,000, holding the other pollutants constant. This is statistically significant with a p-value of 0.00462, meaning there is strong evidence that hydrocarbons are negatively associated with mortality rates.

(e) R-squared 

The adjusted R-squared value of 0.2363 accounts for the number of predictors in the model. Since this value is close to the R-squared value, it indicates that the number of predictors included is not drastically overfitting the data.

(f) F-statistic (7.086, p-value = 0.0004044):

The F-statistic tests the null hypothesis that all coefficients are zero (i.e., none of the predictors have an effect on mortality rates). The p-value (0.0004044) indicates that the overall model is statistically significant, meaning that at least one of the predictors is significantly associated with mortality rates.`

### (e) 
Cross validate: fit the model you chose above to the first half of the data and then predict for the second half. You used all the data to construct the model in (d), so this is not really cross validation, but it gives a sense of how the steps of cross validation can be implemented.

```{r}
set.seed(1)

n <- nrow(pollution_data)

half <- floor(n / 2)
train_data <- pollution_data[1:half, ]
test_data <- pollution_data[(half + 1):n, ]

model_cv <- lm(mort ~ log_nox + log_so2 + log_hc, data = train_data)

predictions <- predict(model_cv, newdata = test_data)

comparison <- data.frame(
  Actual = test_data$mort,
  Predicted = predictions
)

ggplot(comparison, aes(x = Predicted, y = Actual)) +
  geom_point(color = 'blue') +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Predicted Mortality Rate", y = "Actual Mortality Rate") +
  ggtitle("Cross-Validation: Predicted vs Actual Mortality Rate") +
  theme_minimal()

mse <- mean((comparison$Actual - comparison$Predicted)^2)
mse

```

## 12.7 
*Cross validation comparison of models with different transformations of outcomes*: when we compare models with transformed continuous outcomes, we must take into account how the nonlinear transformation warps the continuous outcomes. Follow the procedure used to compare models for the mesquite bushes example on page 202.

### (a) 
Compare models for earnings and for log(earnings) given height and sex as shown in page 84 and 192. Use `earnk` and `log(earnk)` as outcomes.

```{r}
library(rstanarm)
earnings <- read.csv("earnings.csv")
earnings$earnk <- earnings$earn/1000 
fit <- stan_glm(earnk ~ height + male, data=earnings, refresh = 0) 
print(fit)
earnings$'log(earnk)' <- log(earnings$earn)
logmodel <- stan_glm(log(earnk) ~ height + male, data=earnings, subset=earn>0, refresh = 0)
print(logmodel)

```

### (b) 
Compare models from other exercises in this chapter.
```{r}
mesquite <- read.csv("mesquite_cleaned.csv")
head(mesquite)
fit_2 <- stan_glm(weight ~ canopy_height, data=mesquite, refresh = 0)
print(fit_2)
logmodel_2 <- stan_glm(log(weight) ~ canopy_height, data=mesquite, refresh = 0)
print(logmodel_2)

```


## 12.8 
*Log-log transformations*: Suppose that, for a certain population of animals, we can predict log weight from log height as follows:  

* An animal that is 50 centimeters tall is predicted to weigh 10 kg.

* Every increase of 1% in height corresponds to a predicted increase of 2% in weight.

* The weights of approximately 95% of the animals fall within a factor of 1.1 of predicted values.

### (a) 
Give the equation of the regression line and the residual standard deviation of the regression.
$log(W)= -2.39+2log(H)$
RSE=0.02
### (b) 
Suppose the standard deviation of log weights is 20% in this population. What, then, is the $R^{2}$ of the regression model described here?  
0.989
## 12.9 
*Linear and logarithmic transformations*: For a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values $D_i$ and $R_i$. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats. Discuss the advantages and disadvantages of the following measures:  

### (a) 
The simple difference, $D_i - R_i$

Advantage: This is straightforward and easy to interpret. A positive value means the Democratic candidate raised more money, while a negative value means the Republican candidate raised more.

Disadvantage:The absolute difference might miss the relative scale of fundraising.

### (b) 
The ratio, $D_i / R_i$
Advantage: Ratios automatically adjust for scale. 

Disadvantage: Ratios can become highly skewed when one candidate raises significantly more money than the other, which can be problematic in linear models.

### (c) 
The difference on the logarithmic scale, $\log D_i - \log R_i$   
Advantage: It reflects the proportional difference between the candidates' fundraising. It accounts for relative differences while maintaining the interpretability of differences.

Disadvantage: If $D_i$ or $R_i$ is zero, the log difference becomes undefined.
### (d) 
The relative proportion, $D_{i}/(D_{i}+R_{i})$. 
Advantage: Proportions are often used in models predicting probabilities or shares (e.g., logistic regression), making this a natural candidate for vote share predictions.

Disadvantage:  Changes in the proportion become less meaningful as values approach 0 or 1, as the scale becomes compressed. This can lead to loss of sensitivity in extreme cases.

## 12.11
*Elasticity*: An economist runs a regression examining the relations between the average price of cigarettes, $P$, and the quantity purchased, $Q$, across a large sample of counties in the United  States, assuming the functional form, $\log Q=\alpha+\beta \log P$. Suppose the estimate for $\beta$ is 0.3.  Interpret this coefficient. 

The coefficient $\beta$=0.3 means that for a 1% increase in the price of cigarettes, the quantity purchased increases by 0.3%.

In other words, there is a positive elasticity between price and quantity purchased, which is somewhat counterintuitive in the context of typical demand models.

This could imply that in this particular sample or context:

Higher prices are associated with higher quantities sold, which could happen for several reasons, such as specific market dynamics, increased demand in higher-income areas, or other confounding factors.
In general, $\beta$=0.3 means that the relationship between price and quantity purchased is such that a proportional change in price leads to a smaller proportional change in quantity purchased (with a factor of 0.3).

## 12.13
*Building regression models*: Return to the teaching evaluations data from Exercise 10.6. Fit regression models predicting evaluations given many of the inputs in the dataset. Consider interactions, combinations of predictors, and transformations, as appropriate. Consider several  models, discuss in detail the final model that you choose, and also explain why you chose it rather than the others you had considered. 

```{r}

```


## 12.14
Prediction from a fitted regression: Consider one of the fitted models for mesquite leaves, for example `fit_4`, in Section 12.6. Suppose you wish to use this model to make inferences about the average mesquite yield in a new set of trees whose predictors are in data frame called  new_trees. Give R code to obtain an estimate and standard error for this population average. You do not need to make the prediction; just give the code. 

```{r}

```
